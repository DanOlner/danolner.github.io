[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dan Olner’s Data Dispatch",
    "section": "",
    "text": "Open data and code used for ONS subnational data conference\n\n\n\n\n\n\ncode\n\n\nons\n\n\nplanning\n\n\n\n\n\n\n\n\n\nNov 10, 2024\n\n\nDan Olner\n\n\n\n\n\n\n\n\n\n\n\n\nHow to automate your way out of Excel hell & other ONS data wrangling stories (business demography edition)\n\n\n\n\n\n\ncode\n\n\nons\n\n\nfirms\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\nDan Olner\n\n\n\n\n\n\n\n\n\n\n\n\nWhat this situation needs is another blog, said no-one ever\n\n\n\n\n\n\ngumph\n\n\n\n\n\n\n\n\n\nNov 4, 2024\n\n\nDan Olner\n\n\n\n\n\n\n\n\n\n\n\n\nUK trade flows\n\n\n\n\n\n\nfirms\n\n\ngeo\n\n\nio\n\n\n\n\n\n\n\n\n\nNov 26, 2014\n\n\nDan Olner\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "What this situation needs is another blog, said no-one ever",
    "section": "",
    "text": "“Another blog! Thank the Gods! Blogging is so now, isn’t it?”\nThat’s quite enough sarcasm from you. What this lovely website is for:\n\nUsing the ace Quarto blogging platform for writing up data / techie / code / mapping stuff in a much more straightforward way than using Jekyll (the previous github frontend, now archived here). RStudio just makes it for you! With some tweaks. Github repo for this blog is here.\nA place to explain what I’ve done with R projects - not least explaining them to future me. Future me is very forgetful and needs to have things explained very simply to him\nGet down the techie bits behind work I’m doing to support regional economic data analysis, so I can keep that separate from things like the regional economic tools site (also Quarto).\n\n*Links to my github / linkedin / bluesky / wordpress site (or use links up above).\nHere is a picture of a kitten on a unicorn, via here. You’re welcome."
  },
  {
    "objectID": "posts/business_demography/index.html",
    "href": "posts/business_demography/index.html",
    "title": "How to automate your way out of Excel hell & other ONS data wrangling stories (business demography edition)",
    "section": "",
    "text": "I’ve been analysing the latest ONS business demography data (that ONS pull from the IDBR). It contains a tonne of great data on business births, deaths, numbers, ‘high growth’ firms, survival rates, down to local authority level (though sadly sector breakdowns only at national level).\n\nMy working report from that is here - hoping to add more\nPrep code is here\nQuarto code for the report is here\n\n\nGetting data out of Excel documents can be a bit extremely horrible [noting, to be clear, that Excel docs like this are super useful for many people, but just nasty for those of us who want to get the data into something like R or Python, so…]. In this case, what we’ve got is this:\n\nFor each type of data (firm births, deaths, active count, high growth count etc) there are four sheets covering different time periods, with two spanning two years and two with a single year. Why? That’s unclear until you check the geographies - the local authorities (LAs) used don’t match across sheets. Why? Because the boundaries changed, so there’s a different sheet each year they’ve changed.\n\nSo if we want consistent data across all time periods, we’ve got a couple of things to do:\n\nGet the data out of each set of four sheets into one;\nHarmonise the geographies so datapoints are consistent.\n\nLuckily, the LA changes have all been to combine into larger units over time (usually unitary authorities) - so all earlier LAs fit neatly into later boundaries. Phew. This means values from earlier LAs can be summed to the larger/later ones - backcasting 2022 boundaries through all previous data.\nSome anonymous angel/angels made this Wikipedia page clearly laying out when and what local authorities combined into larger unitary ones in recent years. Using that, we can piece together the changes to get to this function that does the harmonising. It groups previous LAs - that only needs to backcast 2021/2022 names once, no faffing around with each separate sheet - and then summarises counts for those new groups, for previous years’ data.\nPrior to that, though, we need to pull the sheets into R. There are a lot of sheets - doing this manually would be baaad…\n\nThe readxl package to the rescue! Part of the tidyverse, it can be used to automate extracting data from any sheet and set of cells in an Excel document. I do that in the function here, specifically for pulling out the correct cells from the ONS demography Excel. That’s used in the code here.\n\n(Image stolen from here)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "“Another blog! Thank the Gods! Blogging is so now, isn’t it?”\nThat’s quite enough sarcasm from you. What this lovely website is for:\n\nUsing the ace Quarto blogging platform for writing up data / techie / code / mapping stuff in a much more straightforward way than using Jekyll (the previous github frontend, now archived here). RStudio just makes it for you! With some tweaks. Github repo for this blog is here.\nA place to explain what I’ve done with R projects - not least explaining them to future me. Future me is very forgetful and needs to have things explained very simply to him.\nGet down the techie bits behind work I’m doing to support regional economic data analysis, so I can keep that separate from things like the regional economic tools site (also Quarto).\n\n*Links to my github / linkedin / bluesky / wordpress site (or use links up above).\nHere is a picture of a kitten on a unicorn, via here. You’re welcome."
  },
  {
    "objectID": "posts/live_project_page/index.html",
    "href": "posts/live_project_page/index.html",
    "title": "Open data and code used for ONS subnational data conference",
    "section": "",
    "text": "Off the back of presenting at the ONS subnational data conference, thought I’d collect (1) the key open data / code bits I used in the slides and (2) a few other open data bits and bobs in the same place, as well as a few extra bits mentioned in there.\nThere’s a mix here of step by step data walkthroughs and raw code: I want to work on getting more of this into a form that’s useable by more people. Part of that is testing what actually is useful and iterating.\nI’ll add in some to-do notes on things that need updating / changing / improving and change this page as those get done."
  },
  {
    "objectID": "posts/live_project_page/index.html#data-and-code-used",
    "href": "posts/live_project_page/index.html#data-and-code-used",
    "title": "Open data and code used for ONS subnational data conference",
    "section": "Data and code used",
    "text": "Data and code used\n\nFor the 1971-81 ‘scarring’ work (used in ‘Steel City: Deindustrialisation and Peripheralisation in Sheffield’ with Jay Emery and Gwilym Pryce):\n\nHarmonised Census data 1971 to 2011: country of birth and employment variables harmonised along with consistent geography. Full explanation in the readme there talks through how to get the data for country of birth (and further down the page there’s a link to the employment data). POSSIBLE TO-DO: HARMONISE WITH 2021 (and 1961???)\nThat data is used in this RMarkdown output that produces the plots used (from this repo with general data stitching code). The data for the RMarkdown output, using the harmonised datasets, is processed in this R Script.\n\nFor the sector proportion plots, and other code on processing ONS regional GVA data for location quotients, mapping and other bits, see this code and data stepthrough on the regecontools site.\nThe productivity “GVA vs JOBS percent change” plots and map don’t have a good walkthrough yet - the code (including code to update to latest BRES and regional GVA data) is here in the repo for the first tranche of sector analysis work done for SYMCA, and is fairly readable and self-contained there. That BRES data is automatically extracted using the super-useful NOMISR package in this script and processed in this script (where it’s linked to the LCREE dataset, along with GVA data - work done in this script and then collated for a report in this Quarto doc). TO-DO: MAKE WALKTHROUGH FOR PROD PLOTS\nThe GVA per hour plot is part of this walkthrough on the regecontools page looking more broadly at some GVA per capita / per hour worked.\nThe Beatty / Fothergill rank change plot is from this fuller breakdown of their data, with code walkthrough, on the regecontools site."
  },
  {
    "objectID": "posts/live_project_page/index.html#other-links",
    "href": "posts/live_project_page/index.html#other-links",
    "title": "Open data and code used for ONS subnational data conference",
    "section": "Other links",
    "text": "Other links\n\nThe Y-PERN website.\nWrite up / blog post of the 2019 Sheffield Data for Good hack day.\nLiverpool City Region Civic Data Coop\nStory on Sheffield Neighbourhood Mapping project (link to current map version).\nCentre for Cities LA Evidential report.\n\nReferences from the presentation:\n\nRice / Venables: “The persistent consequences of adverse shocks: how the 1970s shaped UK regional inequality” here\nSarah Willams, Data Action: Using Data for Public Good.\nMartin A. Schwartz, “The Importance of Stupidity in Scientific Research.” Journal of Cell Science 121.\nPeter Tennant on Bluesky talking about how we grow and why we need an open mind and be willing to be wrong."
  },
  {
    "objectID": "posts/ons_subnational_data_conf/index.html",
    "href": "posts/ons_subnational_data_conf/index.html",
    "title": "Open data and code used for ONS subnational data conference",
    "section": "",
    "text": "Off the back of presenting at the ONS subnational data conference, this post collects the open data / code I used in the slides, as well as a few extra bits mentioned in there.\nThe presentation talks about the huge value and power of ONS data for the UK: how it can help us understand where we’ve come from and where we are now – and so help us work out we want to go.\nThere’s a mix here of step by step data walkthroughs and raw code: I want to work on getting more of this into a form that’s as useable as possible, ideally through testing what actually is useful and iterating.\nI’ll add in some to-do notes on things that need updating / changing / improving and change this page as those get done."
  },
  {
    "objectID": "posts/ons_subnational_data_conf/index.html#data-and-code-used",
    "href": "posts/ons_subnational_data_conf/index.html#data-and-code-used",
    "title": "Open data and code used for ONS subnational data conference",
    "section": "Data and code used",
    "text": "Data and code used\n\nFor the 1971-81 ‘scarring’ work (used in ‘Steel City: Deindustrialisation and Peripheralisation in Sheffield’ with Jay Emery and Gwilym Pryce):\n\nHarmonised Census data 1971 to 2011: country of birth and employment variables harmonised along with consistent geography. Full explanation in the readme there talks through how to get the data for country of birth (and further down the page there’s a link to the employment data). POSSIBLE TO-DO: HARMONISE WITH 2021 (and 1961???)\nThat data is used in this RMarkdown output that produces the plots used (from this repo with general data stitching code). The data for the RMarkdown output, using the harmonised datasets, is processed in this R Script.\n\nFor the sector proportion plots, and other code on processing ONS regional GVA data for location quotients, mapping and other bits, see this code and data stepthrough on the regecontools site.\nThe productivity “GVA vs JOBS percent change” plots and map don’t have a good walkthrough yet - the code (including code to update to latest BRES and regional GVA data) is here in the repo for the first tranche of sector analysis work done for SYMCA, and is fairly readable and self-contained there. That BRES data is automatically extracted using the super-useful NOMISR package in this script and processed in this script (where it’s linked to the LCREE dataset, along with GVA data - work done in this script and then collated for a report in this Quarto doc). TO-DO: MAKE WALKTHROUGH FOR PROD PLOTS\nThe GVA per hour plot is part of this walkthrough on the regecontools page looking more broadly at some GVA per capita / per hour worked.\nThe Beatty / Fothergill rank change plot is from this fuller breakdown of their data, with code walkthrough, on the regecontools site."
  },
  {
    "objectID": "posts/ons_subnational_data_conf/index.html#other-links",
    "href": "posts/ons_subnational_data_conf/index.html#other-links",
    "title": "Open data and code used for ONS subnational data conference",
    "section": "Other links",
    "text": "Other links\n\nThe Y-PERN website.\nSYMCA Plan for Good Growth page, which includes the M-D economic analysis and my own sectoral 3-pager summary.\nWrite up / blog post of the 2019 Sheffield Data for Good hack day.\nLiverpool City Region Civic Data Coop\nStory on Sheffield Neighbourhood Mapping project (link to current map version).\nCentre for Cities LA Evidential report."
  },
  {
    "objectID": "posts/ons_subnational_data_conf/index.html#references-from-the-presentation",
    "href": "posts/ons_subnational_data_conf/index.html#references-from-the-presentation",
    "title": "Open data and code used for ONS subnational data conference",
    "section": "References from the presentation:",
    "text": "References from the presentation:\n\nRice / Venables: “The persistent consequences of adverse shocks: how the 1970s shaped UK regional inequality” here\nSarah Willams, Data Action: Using Data for Public Good.\nMartin A. Schwartz, “The Importance of Stupidity in Scientific Research.” Journal of Cell Science 121.\nPeter Tennant on Bluesky talking about how we grow and why we need an open mind and be willing to be wrong."
  },
  {
    "objectID": "posts/ons_subnational_data_conf/index.html#bits-i-didnt-get-to-leave-in-the-slides",
    "href": "posts/ons_subnational_data_conf/index.html#bits-i-didnt-get-to-leave-in-the-slides",
    "title": "Open data and code used for ONS subnational data conference",
    "section": "Bits I didn’t get to leave in the slides",
    "text": "Bits I didn’t get to leave in the slides\n\nAnalysis of ONS business demography data that links local authorities across the dataset, including business ‘efficiency’ (balance of births and deaths) showing something shifted in more recent years in the south. (I write about automating your way out of an Excel data hole for this project here\nThe incredible Dutch secure data service data used in our Rotterdam project - paper here, supplementary material with a map here. Individual-level data! 100m^2, track over time! Link to other survey data! Secure, trustworthy, easy to use!\nNorthern Irish Census data - summarised down to 100m^2. Allows you to e.g. see Belfast like this (interactive map)."
  },
  {
    "objectID": "posts/ons_subnational_data_conf/index.html#bits-i-didnt-manage-to-cram-in-the-slides",
    "href": "posts/ons_subnational_data_conf/index.html#bits-i-didnt-manage-to-cram-in-the-slides",
    "title": "Open data and code used for ONS subnational data conference",
    "section": "Bits I didn’t manage to cram in the slides",
    "text": "Bits I didn’t manage to cram in the slides\n\nAnalysis of ONS business demography data that links local authorities across the dataset, including business ‘efficiency’ (balance of births and deaths) showing something shifted in more recent years in the south. (I write about automating your way out of an Excel data hole for this project here\nThe incredible Dutch secure data service data used in our Rotterdam project - paper here, supplementary material with a map here. Individual-level data! 100m^2, track over time! Link to other survey data! Secure, trustworthy, easy to use!\nNorthern Irish Census data - summarised down to 100m^2. Allows you to e.g. see Belfast like this (interactive map)."
  },
  {
    "objectID": "posts/uk_intermediate_tradeflows/index.html",
    "href": "posts/uk_intermediate_tradeflows/index.html",
    "title": "UK trade flows",
    "section": "",
    "text": "This is one of the fun things I coded up in the process of developing the last grant I worked on. I’ll explain a bit about it and then share some thoughts on whether it’s any good as a visualisation. There’s a sharper HD version of this video here and a dist.zip file on the github page if you want a play.\nYour standard input-output table takes a bunch of economic sectors and, in a matrix, gives the amounts of money flowing between each of them. For the UK, we’ve got ‘combined use’ matrices that include imported inputs moving between sectors, as well as domestic use only, excluding imports. (These two work with different types of prices, though, so they’re not directly comparable.)\nThis is the boiled-down version of the data I use, from the first data link above: the 2012 combined use matrix. Github gives you a scroll bar at the bottom to view the whole CSV file. The sector names are only in the first column, but they also apply to each column heading along the top. So, for example, the first number column starting with 2822: this is what ‘agriculture, hunting, related services’ spends on other sectors. So the first value is what agriculture spends on itself (it’s in millions of pounds; the matrix diagonal gives the amounts each sector spends on itself.) This is a tip from Anne Owen that’s always helped me: think of each column as a receipt of what that sector has bought. So summing the receipt gives you that sector’s total consumption. Summing each row gives you its total demand - how much others buy from it.\nThe visualisation shows what this matrix looks like if you stick it into a force-based layout and make each money flow a moving circle. The live version is interactive, allowing you to explore sector connections.\nSo: any good as a visualisation? Before I’d produced it, I would have said, mmmm - not really. It’s fun to play with but doesn’t really convey information. It does manage to give a quick overview of the relative size of sectors and how much money moves between them, but you can’t ask it any useful quantitative questions. I’ve since learned a lot more about the internal structure of these IO matrices using R - perhaps that’s something I’ll come back to. I have also coded a ‘random walk centrality’ test (that code is in the source files, though it’s turned off at present) - so it’s certainly possible to use the network structure to do some analysis.\nSomething unexpected happened with this visualisation, though. It engaged people. Prior to this, I probably wouldn’t have thought that was an important thing but, looking back, having something like this that’s able to draw someone in - that’s turned out to be very useful. One of my colleagues used it in a tutorial and apparently they were really taken by it.\nThat kind of initial hook can be enough to make someone want to find out more. That’s been a useful lesson for me. If I were drawing up a criteria list for successful visualisations, this one’s made me think of adding ‘engagement value’ or ‘hook power’ or somesuch. This IO viz has plenty of that. I think it manages to give an impression of the economy as a whole that would otherwise be hard to see. (Though there are reasons to distrust the picture it paints: it tells you construction is by far the biggest sector - it wasn’t until 2013, when ONS took three separate construction sectors and combined them.)\nBut another visualisation criteria should, of course, be ‘does it communicate information effectively?’ This doesn’t manage so well. Perhaps the ideal is to maximise communication / information / hookiness. Perhaps there’s a trade-off there too - making something that might initially make a person go ‘wooooo’ will probably mean, after a few minutes, they’ll realise it’s a bit meaningless.\nEven so: prior to this, it would never have occurred to me that hookiness could be useful in itself. For the grant, this viz helped me say: “look, these are the money flows moving in the UK. We want to want to know where in the UK they move”.\nThis is also a good example of why I still like Java. There’s a lot of work going on there - it would likely run unuseably slow in javascript. This takes us straight back to the ‘wooo/information’ trade-off though. One might argue the computationally intense stuff it’s doing is useless for conveying information - and including it, insisting on a more powerful codebase, is cutting it off from an easily accessible home on the web."
  }
]